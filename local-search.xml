<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>注意力机制</title>
    <link href="/2023/10/20/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2023/10/20/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="注意力提示">1. 注意力提示</h1><p>注意力是一种稀缺的资源，人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。注意力是稀缺的，环境中的干扰注意力的信息不少。整个人类历史中，将注意力引向感兴趣的一小部分信息的能力，使我们的大脑更明智地分配资源来生存、成长和社交。</p><h2 id="生物学中的注意力机制">1.1 生物学中的注意力机制</h2><p>能够基于非自住性提示和自主性提示有选择地引导注意力的焦点：</p><ul><li>非自主性提示：基于环境中的物体的突出性和易见性</li><li>自主性提示：基于主观医院推动的；</li></ul><h2 id="查询键和值">1.2 查询、键和值</h2><p>通过两种注意力提示，用神经网络来设计注意力机制：</p><ul><li>在注意力机制的背景下，将自主性提示成为 <em>查询（Query）</em><ul><li>给定任何查询，注意力机制通过注意力汇聚（AttentionPooling）将选择引导至感官输入（Sensory Inputs）。</li></ul></li><li>在注意力机制中，将感官输入称为<em>值(Value)</em><ul><li>每个值都与一个 <em>键(key)</em>配对，这可以想象为感官输入的非自住提示。</li><li></li></ul></li></ul><figure><img src="./assets/qkv.svg" alt="qkv.svg" /><figcaption aria-hidden="true">qkv.svg</figcaption></figure><ul><li>查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚，注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出</li></ul><h1 id="注意力汇聚">2 注意力汇聚</h1><h2 id="非参数注意力汇聚nadaraya-watson核回归">2.1非参数注意力汇聚：Nadaraya-Watson核回归</h2><ul><li><p>一种简单的估计器解决回归问题：基于平均汇聚来计算所有训练样本输出值的平均值：</p><ul><li><p><span class="math display">\[f(x) = \frac{1}{n}\sum^{n}_{i=1}y_i\]</span></p></li><li><p>这个估计器没有考虑输入 <spanclass="math inline">\(x_i\)</span></p></li></ul></li><li><p>Nadaraya 和 Waston提出了根据输入的位置对输出 <spanclass="math inline">\(y_i\)</span> 进行加权的方法——<em>Nadaraya-Watson核回归</em>：</p><ul><li><p><span class="math display">\[f(x)=\sum^{n}_{i=1}\frac{K(x-x_i)}{\sum^{n}_{j=1}K(x-x_i)}\]</span></p></li><li><p>其中 <span class="math inline">\(K\)</span> 是<em>核(kernel)</em></p></li></ul></li><li><p>重写Nadaraya-Watson核回归形式，使其成为一个更通用的形式如下：</p><ul><li><p><span class="math display">\[f(x)=\sum^{n}_{x=1}\alpha (x,x_i)y_i\]</span></p><ul><li>其中 <span class="math inline">\(x\)</span> 是查询， <spanclass="math inline">\((x_i, y_i)\)</span> 是键值对。式子2中注意力汇聚 是<span class="math inline">\(y_i\)</span> 的加权平均。将查询 <spanclass="math inline">\(x\)</span> 和键 <spanclass="math inline">\(x_i\)</span> 之间的关系建模为<em>注意力权重（Attention Weight</em>） ，这个权重被分配给每一个对应值<span class="math inline">\(y_i\)</span> .</li><li>对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1。</li></ul></li></ul></li><li><p>定义高斯核：</p><ul><li><span class="math display">\[K(u)=\frac{1}{\sqrt{2\pi}}exp(-\frac{u^2}{2})\]</span></li></ul></li><li><p>将高斯核带入式2和式3可得：</p><ul><li><p><span class="math display">\[\begin{split}\begin{aligned} f(x) &amp;=\sum_{i=1}^n \alpha(x, x_i)y_i\\ &amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x -x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)}y_i \\&amp;= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x -x_i)^2\right) y_i. \end{aligned}\end{split}\]</span></p><ul><li>如果一个键 <span class="math inline">\(x_i\)</span>越是接近给定的查询 <span class="math inline">\(x\)</span>，那么分配给这个键对应值 <span class="math inline">\(y_i\)</span>的注意力权重就会越大，也就获得了更多的注意力。</li></ul></li><li><p>因为 <em>Nadaraya-Watson核回归</em>是一个非参数模型，因此式5的模型为非参数的注意力汇聚模型（nonparametricattention pooling）</p></li></ul></li></ul><h2 id="带参数注意力汇聚">2.2 带参数注意力汇聚</h2><p>将可学习的参数 <span class="math inline">\(w\)</span>集成到注意力汇聚中： <span class="math display">\[\begin{split}\begin{aligned}f(x) &amp;= \sum_{i=1}^n \alpha(x, x_i) y_i\\&amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x -x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x -x_j)w)^2\right)} y_i \\&amp;= \sum_{i=1}^n\mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right)y_i.\end{aligned}\end{split}\]</span></p><h1 id="注意力评分函数">3. 注意力评分函数</h1><ul><li>可以将6中的高斯核指数部分视为<em>注意力评分函数（attention scoringfunction）</em>，</li><li>通过式6可以得到与键对应的值的概率分布（即注意力权重）</li></ul><figure><img src="./assets/attention-output.svg"alt="../_images/attention-output.svg" /><figcaptionaria-hidden="true">../_images/attention-output.svg</figcaption></figure><p>用数学语言描述，假设有一个查询 <span class="math inline">\(\mathbf{q}\in \mathbb{R}^q\)</span> 和 m个“键－值”对 <spanclass="math inline">\((\mathbf{k}_1, \mathbf{v}_1), \ldots,(\mathbf{k}_m, \mathbf{v}_m)\)</span>， 其中<spanclass="math inline">\(\mathbf{k}_i \in \mathbb{R}^k\)</span>。注意力汇聚函数f就被表示成值的加权和：</p><ul><li><p><span class="math display">\[f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m,\mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i)\mathbf{v}_i \in \mathbb{R}^v,\]</span></p></li><li><p>其中查询 <span class="math inline">\(q\)</span> 和键 <spanclass="math inline">\(k_i\)</span> 的注意力权重（标量）是通过注意力评分函数 <span class="math inline">\(a\)</span>将两个向量映射成标量， 再经过softmax运算得到的：</p></li><li><p><span class="math display">\[\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q},\mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m\exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}.\]</span></p></li></ul><h2 id="掩蔽softmax操作">3.1 掩蔽softmax操作</h2><p>softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。</p><ul><li>为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度，以便在计算softmax时过滤超出指定范围的位置。</li><li>实现softmax操作（masked softmaxoperation），其中任何超出有效长度的位置都被掩蔽并置为0。</li></ul><h2 id="加性注意力">3.2 加性注意力</h2><p>一般来说，当查询和键是不同长度的矢量时，我们可以使用加性注意力作为评分函数。 给定查询 <spanclass="math inline">\(\mathbf{q} \in \mathbb{R}^q\)</span> 和 键<spanclass="math inline">\(\mathbf{k} \in \mathbb{R}^k\)</span>，<em>加性注意力</em>（additive attention）的评分函数为: <spanclass="math display">\[a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbfW_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},\]</span></p><ul><li>其中可学习的参数时 <span class="math inline">\(\mathbf W_q\in\mathbbR^{h\times q}\)</span> 、<span class="math inline">\(\mathbf W_k \in\mathbb R^{h \times k}\)</span> 和 <span class="math inline">\(\mathbfw_v \in \mathbb R^h\)</span></li><li>将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数h。通过使用tanh作为激活函数，并且禁用偏置项。</li></ul><h2 id="缩放点积注意力">3.3 缩放点积注意力</h2><p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同长度<spanclass="math inline">\(d\)</span>。假设查询和键的所有元素都是独立的随机变量，并满足零均值和单位方差，那么两个向量的点积的均值为0，方差为<span class="math inline">\(d\)</span>。</p><ul><li><p>为了确保无论向量长度如何，点积的方差在不考虑向量长度的情况下任然为1，将点积除以<span class="math inline">\(\sqrt{d}\)</span>，则得到缩放点积注意力（scaled dot-productattention）评分函数为：</p></li><li><p><span class="math display">\[a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}.\]</span></p></li><li><p>在实践中通常从小批量的角度来考虑提高效率，查询和键的长度为 <spanclass="math inline">\(d\)</span>，值的长度为 <spanclass="math inline">\(v_0\)</span> 。查询<spanclass="math inline">\(\mathbf Q\in\mathbb R^{n\times d}\)</span>、键<span class="math inline">\(\mathbf K\in\mathbb R^{m\times d}\)</span>和 值 <span class="math inline">\(\mathbf V\in\mathbb R^{m\timesv}\)</span> 的缩放点积注意力是：</p><ul><li><p><span class="math display">\[\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right)\mathbf V \in \mathbb{R}^{n\times v}.\]</span></p></li><li></li></ul></li></ul><h1 id="bahdanau注意力">4. Bahdanau注意力</h1><p>Bahdanau等人基于Graves的可微注意力模型，将文本字符与更长的笔迹对齐，其中对齐方式仅向一个方向移动的启发，提出了一个没有严格单项对齐限制的<em>可微注意力模型</em>[ <ahref="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id6">Bahdanauet al., 2014</a>]</p><ul><li>在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。<ul><li>这是通过将上下文变量视为注意力集中的输出来实现的</li></ul></li></ul><h2 id="模型">4.1 模型</h2><p>这个新的基于注意力机制模型定义如下： <span class="math display">\[\mathbf{c}_{t&#39;} = \sum_{t=1}^T \alpha(\mathbf{s}_{t&#39; - 1},\mathbf{h}_t) \mathbf{h}_t,\]</span></p><ul><li>其中，时间步 <span class="math inline">\(\mathbf t&#39;-1\)</span>时的解码器器隐状态 <span class="math inline">\(\mathbfs_{t&#39;-1}\)</span> 是查询，编码器隐状态 <spanclass="math inline">\(\mathbf h_t\)</span> 即是键，也是值，注意力权重<span class="math inline">\(\mathbf \alpha\)</span> 是使用 <em>式 9</em>所定义的加性注意力打分函数计算的</li></ul><p>其架构如下图：</p><figure><img src="./assets/seq2seq-attention-details.svg"alt="../_images/seq2seq-attention-details.svg" /><figcaptionaria-hidden="true">../_images/seq2seq-attention-details.svg</figcaption></figure><h1 id="多头注意力">5. 多头注意力</h1><p>为了当给定相同的查询、键和值的集合时，模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖（例如短距离依赖和长距离依赖关系）。因此，允许注意力机制组合起来使用查询、键和值的不同<em>子空间表示 （ representation subspaces ）</em>时有益的。</p><ul><li>解决方法：<ul><li>可以用独立学习得到的h组不同的 <em>线性投影 ( Linear projections)</em> 来变换查询、键和值。</li><li>然后将这h组变换后的查询、键和值并行送到注意力汇聚中，以产生最终的输出。</li><li>这种方法被称为<strong><code>多头注意力机制（multihead attention）</code></strong><ul><li>对于h个注意力汇聚输出，每一个注意力汇聚都被称为一个<em>头（head）</em></li></ul></li></ul></li></ul><figure><img src="./assets/multi-head-attention.svg"alt="../_images/multi-head-attention.svg" /><figcaptionaria-hidden="true">../_images/multi-head-attention.svg</figcaption></figure><h2 id="模型-1">5.1 模型</h2><p>多头注意力机制用数学语言描述如下：</p><ul><li><p>给定查询 <span class="math inline">\(\mathbf{q} \in\mathbb{R}^{d_q}\)</span> 、键 <span class="math inline">\(\mathbf{k}\in \mathbb{R}^{d_k}\)</span> 和 值 <spanclass="math inline">\(\mathbf{v} \in\mathbb{R}^{d_v}\)</span>，每个注意力头 $ (i = 1, , h)$的计算方式为：</p></li><li><p><span class="math display">\[\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbfk,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},\]</span></p><ul><li><p>其中可学习的参数包括 <span class="math inline">\(\mathbfW_i^{(q)}\in\mathbb R^{p_q\times d_q}\)</span> 、<spanclass="math inline">\(\mathbf W_i^{(k)}\in\mathbb R^{p_k\timesd_k}\)</span> 和 <span class="math inline">\(\mathbf W_i^{(v)}\in\mathbbR^{p_v\times d_v}\)</span> ，以及代表注意力汇聚的函数<spanclass="math inline">\(f\)</span> , <spanclass="math inline">\(f\)</span> 可以是3中的加性注意力和缩放点积注意力。 多头注意力的输出需要经过另一个线性转换，它对应着h个头连结后的结果，因此其可学习参数是 <spanclass="math inline">\(\mathbf W_o\in\mathbb R^{p_o\times hp_v}\)</span>：</p></li><li><p><span class="math display">\[\begin{split}\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbfh_h\end{bmatrix} \in \mathbb{R}^{p_o}.\end{split}\]</span></p><ul><li>基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。</li></ul></li></ul></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/10/20/hello-world/"/>
    <url>/2023/10/20/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
