<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>基于LangChain的会话RAG应用</title>
    <link href="/2024/09/05/%E5%9F%BA%E4%BA%8ELangChain%E7%9A%84%E4%BC%9A%E8%AF%9DRAG%E5%BA%94%E7%94%A8/"/>
    <url>/2024/09/05/%E5%9F%BA%E4%BA%8ELangChain%E7%9A%84%E4%BC%9A%E8%AF%9DRAG%E5%BA%94%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="基于langchain的会话rag应用">基于LangChain的会话RAG应用</h1><p>在许多问答应用程序中，希望允许用户进行前后对话，这意味着应用程序需要某种对过去问答的记忆，以及将这些问题纳入当前思考的逻辑。</p><p>有两种基本的实现该应用的方式：<code>Chains</code> 和<code>Agent</code></p><h2 id="准备步骤">准备步骤</h2><h3 id="环境配置">环境配置</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-chroma beautifulsoup4<br></code></pre></td></tr></table></figure><h2 id="chains">Chains</h2><p>使用Ollama+Qwen2:7B作为语言模型</p><h4 id="导入所需要的库">1. 导入所需要的库</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs Python"><span class="hljs-keyword">import</span> bs4<br><span class="hljs-keyword">from</span> langchain <span class="hljs-keyword">import</span> hub<br><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> create_retrieval_chain<br><span class="hljs-keyword">from</span> langchain.chains.combine_documents <span class="hljs-keyword">import</span> create_stuff_documents_chain<br><span class="hljs-keyword">from</span> langchain_chroma <span class="hljs-keyword">import</span> Chroma<br><span class="hljs-keyword">from</span> langchain_community.llms <span class="hljs-keyword">import</span> Ollama<br><span class="hljs-keyword">from</span> langchain_cimmunity.embeddings <span class="hljs-keyword">import</span> OllamaEmbeddings<br><span class="hljs-keyword">from</span> langchain_community.document_loaders <span class="hljs-keyword">import</span> WebBaseLoader<br><span class="hljs-keyword">from</span> langchain_core.prompts <span class="hljs-keyword">import</span> ChatPromptTemplate<br><span class="hljs-keyword">from</span> langchain_openai <span class="hljs-keyword">import</span> OpenAIEmbeddings<br><span class="hljs-keyword">from</span> langchain_text_splitters <span class="hljs-keyword">import</span> RecursiveCharacterTextSplitter<br><br>llm = Ollama(model=<span class="hljs-string">&#x27;qwen2:7b&#x27;</span>)<br></code></pre></td></tr></table></figure><h4 id="对博客内容进行加载分块和索引创建检索器">2.对博客内容进行加载、分块和索引，创建检索器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs Python">loader = WebBaseLoader(<br>    web_paths=(<span class="hljs-string">&quot;https://lilianweng.github.io/posts/2023-06-23-agent/&quot;</span>,),<br>    bs_kwargs=<span class="hljs-built_in">dict</span>(<br>        parse_only=bs4.SoupStrainer(<br>            class_=(<span class="hljs-string">&quot;post-content&quot;</span>, <span class="hljs-string">&quot;post-title&quot;</span>, <span class="hljs-string">&quot;post-header&quot;</span>)<br>        )<br>    ),<br>)<br>docs = loader.load()<br><br>text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="hljs-number">1000</span>, chunk_overlap=<span class="hljs-number">200</span>)<br>splits = text_splitter.split_documents(docs)<br>vectorstore = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(model=<span class="hljs-string">&quot;mofanke/acge_text_embedding&quot;</span>))<br>retriever = vectorstore.as_retriever()<br></code></pre></td></tr></table></figure><h4 id="将检索器组合到问答链中">3. 将检索器组合到问答链中</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python">system_prompt = (<br>    <span class="hljs-string">&quot;You are an assistant for question-answering tasks. &quot;</span><br>    <span class="hljs-string">&quot;Use the following pieces of retrieved context to answer &quot;</span><br>    <span class="hljs-string">&quot;the question. If you don&#x27;t know the answer, say that you &quot;</span><br>    <span class="hljs-string">&quot;don&#x27;t know. Use three sentences maximum and keep the &quot;</span><br>    <span class="hljs-string">&quot;answer concise.&quot;</span><br>    <span class="hljs-string">&quot;\n\n&quot;</span><br>    <span class="hljs-string">&quot;&#123;context&#125;&quot;</span><br>)<br><br>prompt = ChatPromptTemplate.from_messages(<br>    [<br>        (<span class="hljs-string">&quot;system&quot;</span>, system_prompt),<br>        (<span class="hljs-string">&quot;human&quot;</span>, <span class="hljs-string">&quot;&#123;input&#125;&quot;</span>),<br>    ]<br>)<br><br>question_answer_chain = create_stuff_documents_chain(llm, prompt)<br>rag_chain = create_retrieval_chain(retriever, question_answer_chain)<br></code></pre></td></tr></table></figure><h4 id="测试">4. 测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">response = rag_chain.invoke(&#123;<span class="hljs-string">&quot;input&quot;</span>: <span class="hljs-string">&quot;What is Task Decomposition?&quot;</span>&#125;)<br>response[<span class="hljs-string">&quot;answer&quot;</span>]<br></code></pre></td></tr></table></figure><h4 id="加入对话历史">5. 加入对话历史</h4><ul><li><p>先前的步骤时直接使用输入查询来检索相关上下文信息，但时在会话环境中，用户查询可能需要会话上下文才能被理解。</p></li><li><p>现在对先前的程序进行更新</p><ul><li><p>Prompt</p><ul><li>更新提示，以支持将历史消息作为对话输入</li></ul></li><li><p>Contextualizing questions：</p><ul><li><p>增加一个子链以获取最新的用户问题，并根据聊天历史重新表述。</p></li><li><p>可以简单理解为建立一个新的“历史感知”检索器</p></li><li><p>先前：</p><ul><li>``` query--&gt;retriever <figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs xl"><br>- 现在<br><br>  - ```<br>    (<span class="hljs-function"><span class="hljs-title">query</span>, conversation history) --&gt;</span> LLM --&gt; <span class="hljs-function"><span class="hljs-title">rephrased</span> query --&gt;</span> retriever<br></code></pre></td></tr></table></figure></li></ul></li></ul></li></ul></li><li><p>定义一个子链，它可以接受历史消息和最新的用户问题，并在问题引用了历史消息中的任何信息时对其及逆行重构</p><ul><li>使用一个包含名为<code>chat_history</code>的MessagePlaceholder变量的提示符。这样就可以使用<code>chat_history</code>输入键，向提示传递信息列表，这些信息列表将插入系统信息之后、包含最新问题的信息之前</li><li>使用辅助函数<code>create_history_aware_retriever</code>，可以处理chat_histroy为空的情况</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> create_history_aware_retriever<br><span class="hljs-keyword">from</span> langchain_core.prompts <span class="hljs-keyword">import</span> MessagesPlaceholder<br><br>contextualize_q_system_prompt = (<br>    <span class="hljs-string">&quot;Given a chat history and the latest user question &quot;</span><br>    <span class="hljs-string">&quot;which might reference context in the chat history, &quot;</span><br>    <span class="hljs-string">&quot;formulate a standalone question which can be understood &quot;</span><br>    <span class="hljs-string">&quot;without the chat history. Do NOT answer the question, &quot;</span><br>    <span class="hljs-string">&quot;just reformulate it if needed and otherwise return it as is.&quot;</span><br>)<br><br>contextualize_q_prompt = ChatPromptTemplate.from_messages(<br>    [<br>        (<span class="hljs-string">&quot;system&quot;</span>, contextualize_q_system_prompt),<br>        MessagesPlaceholder(<span class="hljs-string">&quot;chat_history&quot;</span>),<br>        (<span class="hljs-string">&quot;human&quot;</span>, <span class="hljs-string">&quot;&#123;input&#125;&quot;</span>),<br>    ]<br>)<br>history_aware_retriever = create_history_aware_retriever(<br>    llm, retriever, contextualize_q_prompt<br>)<br></code></pre></td></tr></table></figure><ul><li>构建完整QA链，将检索器更新为新的<code>history_aware_retriever</code>，并使用<code>create_stuff_documents_chain</code>生成问题解答链，输入键为input、chat_history、context和 answer。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.chains <span class="hljs-keyword">import</span> create_retrieval_chain<br><span class="hljs-keyword">from</span> langchain.chains.combine_documents <span class="hljs-keyword">import</span> create_stuff_documents_chain<br><br>qa_prompt = ChatPromptTemplate.from_messages(<br>    [<br>        (<span class="hljs-string">&quot;system&quot;</span>, system_prompt),<br>        MessagesPlaceholder(<span class="hljs-string">&quot;chat_history&quot;</span>),<br>        (<span class="hljs-string">&quot;human&quot;</span>, <span class="hljs-string">&quot;&#123;input&#125;&quot;</span>),<br>    ]<br>)<br><br><br>question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)<br><br>rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)<br></code></pre></td></tr></table></figure><ul><li>测试</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain_core.messages <span class="hljs-keyword">import</span> AIMessage, HumanMessage<br><br>chat_history = []<br><br>question = <span class="hljs-string">&quot;What is Task Decomposition?&quot;</span><br>ai_msg_1 = rag_chain.invoke(&#123;<span class="hljs-string">&quot;input&quot;</span>: question, <span class="hljs-string">&quot;chat_history&quot;</span>: chat_history&#125;)<br>chat_history.extend(<br>    [<br>        HumanMessage(content=question),<br>        AIMessage(content=ai_msg_1[<span class="hljs-string">&quot;answer&quot;</span>]),<br>    ]<br>)<br><br>second_question = <span class="hljs-string">&quot;What are common ways of doing it?&quot;</span><br>ai_msg_2 = rag_chain.invoke(&#123;<span class="hljs-string">&quot;input&quot;</span>: second_question, <span class="hljs-string">&quot;chat_history&quot;</span>: chat_history&#125;)<br><br><span class="hljs-built_in">print</span>(ai_msg_2[<span class="hljs-string">&quot;answer&quot;</span>])<br></code></pre></td></tr></table></figure><h4 id="对话历史状态管理">对话历史状态管理</h4><ul><li>先前虽然整合了历史对话记录，但是仍然在手工管理历史信息，可以通过LangChain的<code>BaseChatMessageHistory</code>和<code>RunnableWithMessageHistory</code>保存聊天历史记录，并自动插入和更新<ul><li>BaseChatMessageHistory<ul><li>存储聊天历史记录</li></ul></li><li>RunnableWithMessageHistory<ul><li>LCEL 链和 BaseChatMessageHistory的封装程序，可在每次调用后将聊天记录注入输入并更新。</li><li>RunnableWithMessageHistory的实例会为你管理聊天记录。它们接受带有键值（默认为“session_id”）的配置，键值指定了要获取并预置到输入中的对话历史记录，并将输出附加到相同的对话历史记录中。</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain_community.chat_message_histories <span class="hljs-keyword">import</span> ChatMessageHistory<br><span class="hljs-keyword">from</span> langchain_core.chat_history <span class="hljs-keyword">import</span> BaseChatMessageHistory<br><span class="hljs-keyword">from</span> langchain_core.runnables.history <span class="hljs-keyword">import</span> RunnableWithMessageHistory<br><br>store = &#123;&#125;<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_session_history</span>(<span class="hljs-params">session_id: <span class="hljs-built_in">str</span></span>) -&gt; BaseChatMessageHistory:<br>    <span class="hljs-keyword">if</span> session_id <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> store:<br>        store[session_id] = ChatMessageHistory()<br>    <span class="hljs-keyword">return</span> store[session_id]<br><br><br>conversational_rag_chain = RunnableWithMessageHistory(<br>    rag_chain,<br>    get_session_history,<br>    input_messages_key=<span class="hljs-string">&quot;input&quot;</span>,<br>    history_messages_key=<span class="hljs-string">&quot;chat_history&quot;</span>,<br>    output_messages_key=<span class="hljs-string">&quot;answer&quot;</span>,<br>)<br><br>conversational_rag_chain.invoke(<br>    &#123;<span class="hljs-string">&quot;input&quot;</span>: <span class="hljs-string">&quot;What is Task Decomposition?&quot;</span>&#125;,<br>    config=&#123;<br>        <span class="hljs-string">&quot;configurable&quot;</span>: &#123;<span class="hljs-string">&quot;session_id&quot;</span>: <span class="hljs-string">&quot;abc123&quot;</span>&#125;<br>    &#125;,  <span class="hljs-comment"># constructs a key &quot;abc123&quot; in `store`.</span><br>)[<span class="hljs-string">&quot;answer&quot;</span>]<br></code></pre></td></tr></table></figure><ul><li>对话历史记录可以在存储字典中查看：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> store[<span class="hljs-string">&quot;abc123&quot;</span>].messages:<br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(message, AIMessage):<br>        prefix = <span class="hljs-string">&quot;AI&quot;</span><br>    <span class="hljs-keyword">else</span>:<br>        prefix = <span class="hljs-string">&quot;User&quot;</span><br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;prefix&#125;</span>: <span class="hljs-subst">&#123;message.content&#125;</span>\n&quot;</span>)<br></code></pre></td></tr></table></figure><figure><imgsrc="./../../../../../Documents/WeChat/WeChat%20Files/wxid_4fea3uji4won22/FileStorage/File/2024-09/assets/conversational_retrieval_chain-5c7a96abe29e582bc575a0a0d63f86b0.png"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h2 id="agents">Agents</h2><p>Agents利用LLM的推理能力，在执行过程中做出决策。使用Agents可以在检索过程中分担一些自主权。尽管它们的行为与链相比不可预测，但是存在一些优势：</p><ul><li>Agents直接生成检索器的输入，不一定需要明确构建上下文；</li><li>Agents可以执行多个检索步骤以服务查询，或者完全不执行检索步骤（例如，响应用户的通用问候）。</li></ul><h3 id="retrieval-tool">Retrieval tool</h3><p>Agents可以访问“工具”并管理其执行。</p><ul><li>在这种情况下，将检索器转换为一个LangChain工具，以供Agents使用。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langchain.tools.retriever <span class="hljs-keyword">import</span> create_retriever_tool<br><br>tool = create_retriever_tool(<br>    retriever,<br>    <span class="hljs-string">&quot;blog_post_retriever&quot;</span>,<br>    <span class="hljs-string">&quot;Searches and returns excerpts from the Autonomous Agents blog post.&quot;</span>,<br>)<br>tools = [tool]<br></code></pre></td></tr></table></figure><ul><li>工具是LangChain的Runnable对象，并实现了一些通用接口</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp">tool.<span class="hljs-built_in">invoke</span>(<span class="hljs-string">&quot;task decomposition&quot;</span>)<br></code></pre></td></tr></table></figure><h3 id="agent-constructor">Agent constructor</h3><p>使用LangGraph构建代理，LangGraph的好处在于，在高级接口后有一个低级的、高度可控的应用程序接口，以便修改代理逻辑时使用。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> langgraph.prebuilt <span class="hljs-keyword">import</span> create_react_agent<br><br>agent_executor = create_react_agent(llm, tools)<br><br><span class="hljs-keyword">from</span> langgraph.checkpoint.memory <span class="hljs-keyword">import</span> MemorySaver<br><br>memory = MemorySaver()<br><br>agent_executor = create_react_agent(llm, tools, checkpointer=memory)<br><br>config = &#123;<span class="hljs-string">&quot;configurable&quot;</span>: &#123;<span class="hljs-string">&quot;thread_id&quot;</span>: <span class="hljs-string">&quot;abc123&quot;</span>&#125;&#125;<br><br><span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> agent_executor.stream(<br>    &#123;<span class="hljs-string">&quot;messages&quot;</span>: [HumanMessage(content=<span class="hljs-string">&quot;Hi! I&#x27;m bob&quot;</span>)]&#125;, config=config<br>):<br>    <span class="hljs-built_in">print</span>(s)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;----&quot;</span>)<br>    <br>query = <span class="hljs-string">&quot;What is Task Decomposition?&quot;</span><br><br><span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> agent_executor.stream(<br>    &#123;<span class="hljs-string">&quot;messages&quot;</span>: [HumanMessage(content=query)]&#125;, config=config<br>):<br>    <span class="hljs-built_in">print</span>(s)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;----&quot;</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>GraphRAG</title>
    <link href="/2024/09/05/GraphRAG/"/>
    <url>/2024/09/05/GraphRAG/</url>
    
    <content type="html"><![CDATA[<h1 id="graphrag">GraphRAG</h1><h2 id="graphrag与传统rag的区别">GraphRAG与传统RAG的区别</h2><h3 id="为什么会有graphrag">为什么会有GraphRAG</h3><h4 id="rag解决的问题">RAG解决的问题</h4><ul><li>使用检索增强生成的RAG技术从外部知识源检索相关信息，使大语言模型（LLMs）能够回答涉及私有或之前从未见过文档集合的问题。</li></ul><h4 id="先前rag">先前RAG</h4><ul><li>解决外部知识的详情介绍，某些细节的检索</li><li>无法解决QFS（query focused summarization）的问题<ul><li>文章的主要主题是什么<ul><li>跨越多个段落，甚至多个文章</li></ul></li></ul></li></ul><h3 id="graphrag的基本概念">GraphRAG的基本概念</h3><ul><li>Document（文档）<ul><li>系统中的输入文档<ul><li>代表CSV中单独的行或单独的txt文件</li></ul></li></ul></li><li>TextUnit（文本块）<ul><li>要分析的文本块<ul><li>这些块的大小、重叠以及它们是否遵守任何数据边界可以进行配置</li><li>CHUNK_BY_COLUMNS为id，以便文档和TextUnits之间存在一对多关系，而不是多对多关系。</li></ul></li></ul></li><li>Entity（实体）<ul><li>从TextUnit中提取的实体。这些实体代表人物、地点、事件等。<ul><li>可以提供实体模型</li></ul></li></ul></li><li>Relationship（关系）<ul><li>两个实体之间的关系，这些关系由协变量生成</li></ul></li><li>Covariate（协变量）<ul><li>提取的声明信息，其中包含可能受事件限制的实体的陈述</li></ul></li><li>Claim（声明）<ul><li>代表具有评估状态和事件限制的积极事实陈述，以协变量的称呼在各处使用</li></ul></li><li>Community Report（社区报告）<ul><li>一旦生成实体，就对其执行分层社区检测，并为该层次结构中的每个社区生成报告。</li></ul></li><li>Node（节点）<ul><li>包含已嵌入和聚集的实体和文档的呈现图形视图的布局信息</li></ul></li></ul><h2 id="graphrag索引构建">GraphRAG索引构建</h2><h3 id="compose-textunit-组成文本单元">1. Compose textunit组成文本单元</h3><p>将文档转换为多个文本单元，之间存在一对多的关系。</p><h4 id="切块技巧">切块技巧</h4><ul><li>切块大小chunk size： 1200 token<ul><li>较大的块会导致输出保真度较低，参考文本意义较小</li><li>较大的块可以大大缩短处理的时间</li></ul></li></ul><h3 id="graph-extraction-图抽取">2. Graph Extraction 图抽取</h3><pre><code class=" mermaid">---title: Graph Extraction---flowchart LRa[TextUnit]--&gt;b[Graph Extraction];b--&gt;c[Graph Summarization];a--&gt;d[claim Extraction];c--&gt;e[Entity Resolution]</code></pre><h4 id="graph-extraction功能">Graph Extraction功能</h4><ul><li>分析每个文本单元，并提取图形基元<ul><li>实体</li><li>关系</li><li>声明</li></ul></li></ul><ol type="1"><li>实体和关系抽取：<ul><li>使用LLM从原始文本中提取实体和关系，包含具有名称、类型和描述实体元组</li><li>具有源、目标和描述的关系元组</li></ul></li><li>实体和关系摘要：<ul><li>通过LLM为每个实体和关系提供简短的摘要描述</li></ul></li><li>Claim Extraction &amp; Emission<ul><li>声明代表具有评估状态和时间限制的积极事实陈述，以协变量的称呼在各处使用</li></ul></li></ol><h3 id="graph-augmentation-图增强">3. Graph Augmentation 图增强</h3><pre><code class=" mermaid">---title: Graph Augmentation---flowchart LRa[Leiden Hierarchical Community Detection]--&gt;b[Node2Vec Graph Embedding];b--&gt;c[Graph Table Emission]</code></pre><ul><li>目的及作用：了解上一步所构建图谱的群落结构，并用更多的信息来扩充图谱<ul><li>了解图的拓扑结构的显式（community）和隐式（Embedding）方法</li></ul></li></ul><h4 id="community-detection">Community Detection</h4><ul><li>使用LeidenHierarchical（层次莱顿）算法来生成实体community的层次结构。<ul><li>对图应用递归群落聚类，直到达到community规模阈值。</li><li>能够提供一种在不同粒度水平上导航和总结图的方法，以供了解图的结构。</li></ul></li></ul><h4 id="graph-embedding">Graph Embedding</h4><ul><li>使用Node2Vec算法生成图的矢量表示。<ul><li>提供一个额外的向量空间来了解图的隐式结构，以便在查询阶段搜索相关概念。</li></ul></li></ul><h4 id="graph-table-emission">Graph Table Emission</h4><ul><li>在完成上述步骤后，最终的实体表和关系表就会在文本字段嵌入发布<ul><li>用于理解构建的community和实体及关系存在关联的关系</li></ul></li></ul><h3 id="community-summarization">4. Community Summarization</h3><pre><code class=" mermaid">---title: Community Summarization---flowchart LRa[Generate Community Reports]--&gt;b[Summarize Communtiy Reports]--&gt;c[Community Embedding]--&gt;d[Community Table Emission];</code></pre><p>目的：</p><ul><li>已community数据为基础，为每个community生成报告，提供图在几个粒度点上对整体图的一个高层次了解。<ul><li>如果community A是顶级社区，将获得有关整个图表的报告</li><li>如果社区是较低级别的，可以获得有关本地集群的报告</li></ul></li></ul><h4 id="generate-community-reports">Generate Community Reports</h4><ul><li>在这一步中，使用LLM生成每个community的摘要。<ul><li>能够了解每个社区所包含的不同信息，并从高层或低层的角度提供对图的范围理解。</li><li>这些报告包含了执行概述，参考了community子结构中的关键实体、关系和声明</li></ul></li></ul><h4 id="summarize-community-reports">Summarize Community Reports</h4><ul><li>在这一步中，每个community报告都会通过LLM进行总结，以便快速使用</li></ul><h4 id="community-embedding">Community Embedding</h4><ul><li>在这一步中，通过生成community reports、community reportsummary和community report标题的文本嵌入来生成community的矢量表示。</li></ul><h4 id="community-tables-emission">Community Tables Emission</h4><ul><li>执行记录工作，输出community table和community report table</li></ul><h3 id="document-processing">5. Document Processing</h3><pre><code class=" mermaid">---title: Documnet Processing---flowchart LRa[Augment]--&gt;b[Link to TextUnits]--&gt;c[Avg. Embedding]--&gt;d[Document Table Emission];</code></pre><p>目的：</p><ul><li>为知识模型创建文档表</li></ul><h4 id="augment-with-columnscsv-only">Augment with Columns（CSVOnly)</h4><ul><li>如果工作流程是在CSV数据上运行，可以配置工作流程，为文档输出添加附加字段，这些字段应存储在输入的CSV表中</li></ul><h4 id="link-to-textunits">Link to TextUnits</h4><ul><li>将每个文档链接到第一阶段创建的文本单元<ul><li>可以了解文档与哪些文本单元相关</li></ul></li></ul><h4 id="document-embedding">Document Embedding</h4><ul><li>使用文档切片的平均嵌入来生成文档的向量表示。</li><li>对没有重叠的文档重新分块，然后为每个分块生成一个嵌入</li><li>根据标记数加权创建这些分块的平均值，并将其用作文档嵌入。<ul><li>能够理解文档之间的隐含关系，帮助生成文档的网络表示</li></ul></li></ul><h4 id="documents-table-emission">Documents Table Emission</h4><ul><li>将文档表放到知识模型中</li></ul><h3 id="network-visualzation">6. Network Visualzation</h3><pre><code class=" mermaid">---title: Network Visualization Workflows---flowchart LRa[Umap Documents]--&gt;b[Umap Entities]--&gt;c[Nodes Table Emission]</code></pre><p>目的：</p><ul><li>支持现有图中实现高维向量空间的网络可视化。</li><li>两个逻辑图在起作用<ul><li>实体关系图</li><li>文档图</li></ul></li></ul><p>方法：</p><ul><li>对于每个逻辑图，都要执行UMAP降维，以生成图的二维表示法。<ul><li>对图在二维空间中可视化，以了解图中节点之间的关系</li></ul></li><li>将UMAP嵌入以节点表的形式发布<ul><li>该表的各行包括了一个判别器，指示节点是文档还是实体，及其UMAP坐标</li></ul></li></ul><h2 id="graphrag-查询">GraphRAG 查询</h2><h3 id="local-search">Local Search</h3><ul><li>本地搜索方法将知识图谱中的结构化数据与输入文档中的非结构化数据相结合，以在查询时利用相关实体信息增强LLM上下文。</li><li>Local查询适合回答：<code>需要理解输入文档中提到的特定实体的问题</code></li></ul><figure><imgsrc="./../../../../../Documents/WeChat/WeChat%20Files/wxid_4fea3uji4won22/FileStorage/File/2024-09/assets/image-20240904153505016.png"alt="image-20240904153505016" /><figcaption aria-hidden="true">image-20240904153505016</figcaption></figure><ul><li>给定用户查询和对话历史，本地搜索方法就能从知识图谱中识别出一组与用户输入语义相关的实体。</li><li>这些实体可作为进入知识图谱的入口，从而进一步提取相关细节，<ul><li>如关联实体、关系、实体协变量和community 报告</li></ul></li><li>能从原始输入文档中提取与已识别实体相关的文本块，然后对这些候选数据源进行优先排序和过滤，使其符合预定义大小的单个上下文窗口，用于生成对用户查询的回复</li></ul><h3 id="global-search">Global Search</h3><p>先前RAG方法在处理需要汇总整个数据集的信息才能得出答案的查询时非常吃力，因为先前RAG依赖对数据集中语义相似的文本内容进行矢量搜索。查询中没有任何内容可以引导它找到正确信息</p><p>GraphRAG能够回答这类问题。LLM生成的知识图谱结构能够告诉我们整个数据集的结构以及主题。</p><ul><li>可以将私有数据集组织成有意义的语义集群，并预先加以总结。</li><li>利用全局搜索方法，LLM在响应用户查询时会使用这类聚类来总结主题。</li></ul><figure><imgsrc="./../../../../../Documents/WeChat/WeChat%20Files/wxid_4fea3uji4won22/FileStorage/File/2024-09/assets/image-20240904154941216.png"alt="image-20240904154941216" /><figcaption aria-hidden="true">image-20240904154941216</figcaption></figure><ul><li>给定用户查询以及可选的对话历史，全局搜索方法使用从图的社区层次结构中指定级别生成的一组LLM生成的社区报告作为上下文数据，以Map-Reduce的方式生成响应。<ul><li>在Map步骤中，社区报告被分割成预定义大小的文本块，然后每个文本块都被用来生成一个中间回复。<ul><li>包含一个点列表，每个点都附有一个表示该点重要性的数字评级。</li></ul></li><li>在Reduce步骤中，从中间回复中筛选出一组最重要的点进行汇总，并以此为上下文生成最终回复。</li></ul></li><li>全局搜索的响应质量在很大程度上会受到为获取社区报告而选择的社区层级的影响。<ul><li>较低层次的报告内容详细，往往能得到更全面的回复，但由于报告数量庞大，也可能会增加生成最终回复所需的时间和LLM 资源。</li></ul></li></ul><blockquote><p>Map-Reduce是一种用于处理大规模数据的并行计算编程模型。它最初由Google提出，用于在分布式计算环境中处理海量数据。Map-Reduce模型分为两个主要阶段：Map阶段和Reduce阶段。</p><h3 id="map阶段">Map阶段</h3><ol type="1"><li><strong>Map函数</strong>：在Map阶段，输入数据集被划分成若干个小块，每个小块由一个Map任务处理。Map函数接受输入数据，并将其转换为一系列键值对（key-valuepairs）。</li><li><strong>分片处理</strong>：Map任务并行处理这些小块数据，生成中间结果。每个Map任务独立运行，不依赖于其他Map任务。</li></ol><h3 id="shuffle阶段">Shuffle阶段</h3><ol type="1"><li><strong>数据分发</strong>：在Map阶段完成后，Map-Reduce框架会将所有中间结果按照键值进行排序，并将具有相同键值的数据传递给同一个Reduce任务。</li><li><strong>数据传输</strong>：这个过程称为Shuffle。它涉及数据的重新分布和传输，以便将具有相同键的数据发送到同一个Reduce任务。</li></ol><h3 id="reduce阶段">Reduce阶段</h3><ol type="1"><li><strong>Reduce函数</strong>：Reduce阶段的每个Reduce任务接收来自Map阶段的中间结果，对具有相同键值的数据执行归约操作，生成最终的输出结果。</li><li><strong>结果输出</strong>：Reduce任务的输出通常会被写入持久化存储，比如分布式文件系统（如HDFS）或数据库中。</li></ol><h3 id="特点和优势">特点和优势</h3><ul><li><strong>容错性</strong>：Map-Reduce框架具有高度的容错性，可以处理节点故障和数据丢失。</li><li><strong>可伸缩性</strong>：适用于处理大规模数据集，可以通过增加计算节点来实现横向扩展。</li><li><strong>简单性</strong>：开发人员只需关注Map和Reduce函数的实现，而不必担心并行化、数据分发等复杂细节。</li><li><strong>通用性</strong>：适用于各种数据处理任务，如日志分析、数据挖掘、机器学习等。</li></ul><h3 id="应用场景">应用场景</h3><ul><li><strong>数据清洗和转换</strong>：处理原始数据，将其转换为可分析的格式。</li><li><strong>数据聚合</strong>：对大规模数据进行统计、汇总等操作。</li><li><strong>搜索引擎索引构建</strong>：构建搜索引擎索引需要处理大量文本数据。</li><li><strong>日志分析</strong>：分析大量服务器日志以提取有用信息。</li></ul><p>Map-Reduce模型的成功促使了许多开源实现，如Apache Hadoop和ApacheSpark，这些工具使得大规模数据处理变得更加容易和高效。</p></blockquote><h2 id="修改graphrag适配开源模型">修改GraphRAG适配开源模型</h2><h3 id="方案">方案</h3><ol type="1"><li>开发并行OpenAI调用逻辑<ul><li>根据调用接口修改<code>LLMParamter()</code>变量</li></ul></li><li>A-&gt;B-&gt;C, <code>call_llm()</code><ul><li><code>call_llm</code>实际调用大模型时堵截</li><li>初始化，环境变量读取，llm实例注册，通过全局变量保存大模型实例，避免每次调用都要生成实例</li><li>factories，实例化不同LLM实例</li></ul></li></ol><h3 id="需要修改的环节">需要修改的环节</h3><ol type="1"><li>settings.yaml<ul><li>模型名称</li><li>模型平台</li></ul></li><li>.env</li><li>llm，embedding调用适配修改</li><li>支持平台的管理，factories</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>注意力机制</title>
    <link href="/2023/10/20/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2023/10/20/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="注意力提示">1. 注意力提示</h1><p>注意力是一种稀缺的资源，人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。注意力是稀缺的，环境中的干扰注意力的信息不少。整个人类历史中，将注意力引向感兴趣的一小部分信息的能力，使我们的大脑更明智地分配资源来生存、成长和社交。</p><h2 id="生物学中的注意力机制">1.1 生物学中的注意力机制</h2><p>能够基于非自住性提示和自主性提示有选择地引导注意力的焦点：</p><ul><li>非自主性提示：基于环境中的物体的突出性和易见性</li><li>自主性提示：基于主观医院推动的；</li></ul><h2 id="查询键和值">1.2 查询、键和值</h2><p>通过两种注意力提示，用神经网络来设计注意力机制：</p><ul><li>在注意力机制的背景下，将自主性提示成为 <em>查询（Query）</em><ul><li>给定任何查询，注意力机制通过注意力汇聚（AttentionPooling）将选择引导至感官输入（Sensory Inputs）。</li></ul></li><li>在注意力机制中，将感官输入称为<em>值(Value)</em><ul><li>每个值都与一个 <em>键(key)</em>配对，这可以想象为感官输入的非自住提示。</li><li></li></ul></li></ul><figure><img src="./assets/qkv.svg" alt="qkv.svg" /><figcaption aria-hidden="true">qkv.svg</figcaption></figure><ul><li>查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚，注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出</li></ul><h1 id="注意力汇聚">2 注意力汇聚</h1><h2 id="非参数注意力汇聚nadaraya-watson核回归">2.1非参数注意力汇聚：Nadaraya-Watson核回归</h2><ul><li><p>一种简单的估计器解决回归问题：基于平均汇聚来计算所有训练样本输出值的平均值：</p><ul><li><p><span class="math display">\[f(x) = \frac{1}{n}\sum^{n}_{i=1}y_i\]</span></p></li><li><p>这个估计器没有考虑输入 <spanclass="math inline">\(x_i\)</span></p></li></ul></li><li><p>Nadaraya 和 Waston提出了根据输入的位置对输出 <spanclass="math inline">\(y_i\)</span> 进行加权的方法——<em>Nadaraya-Watson核回归</em>：</p><ul><li><p><span class="math display">\[f(x)=\sum^{n}_{i=1}\frac{K(x-x_i)}{\sum^{n}_{j=1}K(x-x_i)}\]</span></p></li><li><p>其中 <span class="math inline">\(K\)</span> 是<em>核(kernel)</em></p></li></ul></li><li><p>重写Nadaraya-Watson核回归形式，使其成为一个更通用的形式如下：</p><ul><li><p><span class="math display">\[f(x)=\sum^{n}_{x=1}\alpha (x,x_i)y_i\]</span></p><ul><li>其中 <span class="math inline">\(x\)</span> 是查询， <spanclass="math inline">\((x_i, y_i)\)</span> 是键值对。式子2中注意力汇聚 是<span class="math inline">\(y_i\)</span> 的加权平均。将查询 <spanclass="math inline">\(x\)</span> 和键 <spanclass="math inline">\(x_i\)</span> 之间的关系建模为<em>注意力权重（Attention Weight</em>） ，这个权重被分配给每一个对应值<span class="math inline">\(y_i\)</span> .</li><li>对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布：它们是非负的，并且总和为1。</li></ul></li></ul></li><li><p>定义高斯核：</p><ul><li><span class="math display">\[K(u)=\frac{1}{\sqrt{2\pi}}exp(-\frac{u^2}{2})\]</span></li></ul></li><li><p>将高斯核带入式2和式3可得：</p><ul><li><p><span class="math display">\[\begin{split}\begin{aligned} f(x) &amp;=\sum_{i=1}^n \alpha(x, x_i)y_i\\ &amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}(x -x_i)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)}y_i \\&amp;= \sum_{i=1}^n \mathrm{softmax}\left(-\frac{1}{2}(x -x_i)^2\right) y_i. \end{aligned}\end{split}\]</span></p><ul><li>如果一个键 <span class="math inline">\(x_i\)</span>越是接近给定的查询 <span class="math inline">\(x\)</span>，那么分配给这个键对应值 <span class="math inline">\(y_i\)</span>的注意力权重就会越大，也就获得了更多的注意力。</li></ul></li><li><p>因为 <em>Nadaraya-Watson核回归</em>是一个非参数模型，因此式5的模型为非参数的注意力汇聚模型（nonparametricattention pooling）</p></li></ul></li></ul><h2 id="带参数注意力汇聚">2.2 带参数注意力汇聚</h2><p>将可学习的参数 <span class="math inline">\(w\)</span>集成到注意力汇聚中： <span class="math display">\[\begin{split}\begin{aligned}f(x) &amp;= \sum_{i=1}^n \alpha(x, x_i) y_i\\&amp;= \sum_{i=1}^n \frac{\exp\left(-\frac{1}{2}((x -x_i)w)^2\right)}{\sum_{j=1}^n \exp\left(-\frac{1}{2}((x -x_j)w)^2\right)} y_i \\&amp;= \sum_{i=1}^n\mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right)y_i.\end{aligned}\end{split}\]</span></p><h1 id="注意力评分函数">3. 注意力评分函数</h1><ul><li>可以将6中的高斯核指数部分视为<em>注意力评分函数（attention scoringfunction）</em>，</li><li>通过式6可以得到与键对应的值的概率分布（即注意力权重）</li></ul><figure><img src="./assets/attention-output.svg"alt="../_images/attention-output.svg" /><figcaptionaria-hidden="true">../_images/attention-output.svg</figcaption></figure><p>用数学语言描述，假设有一个查询 <span class="math inline">\(\mathbf{q}\in \mathbb{R}^q\)</span> 和 m个“键－值”对 <spanclass="math inline">\((\mathbf{k}_1, \mathbf{v}_1), \ldots,(\mathbf{k}_m, \mathbf{v}_m)\)</span>， 其中<spanclass="math inline">\(\mathbf{k}_i \in \mathbb{R}^k\)</span>。注意力汇聚函数f就被表示成值的加权和：</p><ul><li><p><span class="math display">\[f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m,\mathbf{v}_m)) = \sum_{i=1}^m \alpha(\mathbf{q}, \mathbf{k}_i)\mathbf{v}_i \in \mathbb{R}^v,\]</span></p></li><li><p>其中查询 <span class="math inline">\(q\)</span> 和键 <spanclass="math inline">\(k_i\)</span> 的注意力权重（标量）是通过注意力评分函数 <span class="math inline">\(a\)</span>将两个向量映射成标量， 再经过softmax运算得到的：</p></li><li><p><span class="math display">\[\alpha(\mathbf{q}, \mathbf{k}_i) = \mathrm{softmax}(a(\mathbf{q},\mathbf{k}_i)) = \frac{\exp(a(\mathbf{q}, \mathbf{k}_i))}{\sum_{j=1}^m\exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}.\]</span></p></li></ul><h2 id="掩蔽softmax操作">3.1 掩蔽softmax操作</h2><p>softmax操作用于输出一个概率分布作为注意力权重。在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。</p><ul><li>为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度，以便在计算softmax时过滤超出指定范围的位置。</li><li>实现softmax操作（masked softmaxoperation），其中任何超出有效长度的位置都被掩蔽并置为0。</li></ul><h2 id="加性注意力">3.2 加性注意力</h2><p>一般来说，当查询和键是不同长度的矢量时，我们可以使用加性注意力作为评分函数。 给定查询 <spanclass="math inline">\(\mathbf{q} \in \mathbb{R}^q\)</span> 和 键<spanclass="math inline">\(\mathbf{k} \in \mathbb{R}^k\)</span>，<em>加性注意力</em>（additive attention）的评分函数为: <spanclass="math display">\[a(\mathbf q, \mathbf k) = \mathbf w_v^\top \text{tanh}(\mathbfW_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},\]</span></p><ul><li>其中可学习的参数时 <span class="math inline">\(\mathbf W_q\in\mathbbR^{h\times q}\)</span> 、<span class="math inline">\(\mathbf W_k \in\mathbb R^{h \times k}\)</span> 和 <span class="math inline">\(\mathbfw_v \in \mathbb R^h\)</span></li><li>将查询和键连结起来后输入到一个多层感知机（MLP）中，感知机包含一个隐藏层，其隐藏单元数是一个超参数h。通过使用tanh作为激活函数，并且禁用偏置项。</li></ul><h2 id="缩放点积注意力">3.3 缩放点积注意力</h2><p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同长度<spanclass="math inline">\(d\)</span>。假设查询和键的所有元素都是独立的随机变量，并满足零均值和单位方差，那么两个向量的点积的均值为0，方差为<span class="math inline">\(d\)</span>。</p><ul><li><p>为了确保无论向量长度如何，点积的方差在不考虑向量长度的情况下任然为1，将点积除以<span class="math inline">\(\sqrt{d}\)</span>，则得到缩放点积注意力（scaled dot-productattention）评分函数为：</p></li><li><p><span class="math display">\[a(\mathbf q, \mathbf k) = \mathbf{q}^\top \mathbf{k}  /\sqrt{d}.\]</span></p></li><li><p>在实践中通常从小批量的角度来考虑提高效率，查询和键的长度为 <spanclass="math inline">\(d\)</span>，值的长度为 <spanclass="math inline">\(v_0\)</span> 。查询<spanclass="math inline">\(\mathbf Q\in\mathbb R^{n\times d}\)</span>、键<span class="math inline">\(\mathbf K\in\mathbb R^{m\times d}\)</span>和 值 <span class="math inline">\(\mathbf V\in\mathbb R^{m\timesv}\)</span> 的缩放点积注意力是：</p><ul><li><p><span class="math display">\[\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right)\mathbf V \in \mathbb{R}^{n\times v}.\]</span></p></li><li></li></ul></li></ul><h1 id="bahdanau注意力">4. Bahdanau注意力</h1><p>Bahdanau等人基于Graves的可微注意力模型，将文本字符与更长的笔迹对齐，其中对齐方式仅向一个方向移动的启发，提出了一个没有严格单项对齐限制的<em>可微注意力模型</em>[ <ahref="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id6">Bahdanauet al., 2014</a>]</p><ul><li>在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。<ul><li>这是通过将上下文变量视为注意力集中的输出来实现的</li></ul></li></ul><h2 id="模型">4.1 模型</h2><p>这个新的基于注意力机制模型定义如下： <span class="math display">\[\mathbf{c}_{t&#39;} = \sum_{t=1}^T \alpha(\mathbf{s}_{t&#39; - 1},\mathbf{h}_t) \mathbf{h}_t,\]</span></p><ul><li>其中，时间步 <span class="math inline">\(\mathbf t&#39;-1\)</span>时的解码器器隐状态 <span class="math inline">\(\mathbfs_{t&#39;-1}\)</span> 是查询，编码器隐状态 <spanclass="math inline">\(\mathbf h_t\)</span> 即是键，也是值，注意力权重<span class="math inline">\(\mathbf \alpha\)</span> 是使用 <em>式 9</em>所定义的加性注意力打分函数计算的</li></ul><p>其架构如下图：</p><figure><img src="./assets/seq2seq-attention-details.svg"alt="../_images/seq2seq-attention-details.svg" /><figcaptionaria-hidden="true">../_images/seq2seq-attention-details.svg</figcaption></figure><h1 id="多头注意力">5. 多头注意力</h1><p>为了当给定相同的查询、键和值的集合时，模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖（例如短距离依赖和长距离依赖关系）。因此，允许注意力机制组合起来使用查询、键和值的不同<em>子空间表示 （ representation subspaces ）</em>时有益的。</p><ul><li>解决方法：<ul><li>可以用独立学习得到的h组不同的 <em>线性投影 ( Linear projections)</em> 来变换查询、键和值。</li><li>然后将这h组变换后的查询、键和值并行送到注意力汇聚中，以产生最终的输出。</li><li>这种方法被称为<strong><code>多头注意力机制（multihead attention）</code></strong><ul><li>对于h个注意力汇聚输出，每一个注意力汇聚都被称为一个<em>头（head）</em></li></ul></li></ul></li></ul><figure><img src="./assets/multi-head-attention.svg"alt="../_images/multi-head-attention.svg" /><figcaptionaria-hidden="true">../_images/multi-head-attention.svg</figcaption></figure><h2 id="模型-1">5.1 模型</h2><p>多头注意力机制用数学语言描述如下：</p><ul><li><p>给定查询 <span class="math inline">\(\mathbf{q} \in\mathbb{R}^{d_q}\)</span> 、键 <span class="math inline">\(\mathbf{k}\in \mathbb{R}^{d_k}\)</span> 和 值 <spanclass="math inline">\(\mathbf{v} \in\mathbb{R}^{d_v}\)</span>，每个注意力头 $ (i = 1, , h)$的计算方式为：</p></li><li><p><span class="math display">\[\mathbf{h}_i = f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbfk,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},\]</span></p><ul><li><p>其中可学习的参数包括 <span class="math inline">\(\mathbfW_i^{(q)}\in\mathbb R^{p_q\times d_q}\)</span> 、<spanclass="math inline">\(\mathbf W_i^{(k)}\in\mathbb R^{p_k\timesd_k}\)</span> 和 <span class="math inline">\(\mathbf W_i^{(v)}\in\mathbbR^{p_v\times d_v}\)</span> ，以及代表注意力汇聚的函数<spanclass="math inline">\(f\)</span> , <spanclass="math inline">\(f\)</span> 可以是3中的加性注意力和缩放点积注意力。 多头注意力的输出需要经过另一个线性转换，它对应着h个头连结后的结果，因此其可学习参数是 <spanclass="math inline">\(\mathbf W_o\in\mathbb R^{p_o\times hp_v}\)</span>：</p></li><li><p><span class="math display">\[\begin{split}\mathbf W_o \begin{bmatrix}\mathbf h_1\\\vdots\\\mathbfh_h\end{bmatrix} \in \mathbb{R}^{p_o}.\end{split}\]</span></p><ul><li>基于这种设计，每个头都可能会关注输入的不同部分，可以表示比简单加权平均值更复杂的函数。</li></ul></li></ul></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/10/20/hello-world/"/>
    <url>/2023/10/20/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your veryfirst post. Check <a href="https://hexo.io/docs/">documentation</a> formore info. If you get any problems when using Hexo, you can find theanswer in <ahref="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> oryou can ask me on <ahref="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <ahref="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
