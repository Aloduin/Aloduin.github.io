<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>注意力机制</title>
    <link href="/2023/10/20/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2023/10/20/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="1-注意力提示"><a href="#1-注意力提示" class="headerlink" title="1. 注意力提示"></a>1. 注意力提示</h1><p>注意力是一种稀缺的资源，人类的注意力被视为可以交换的、有限的、有价值的且稀缺的商品。注意力是稀缺的，环境中的干扰注意力的信息不少。整个人类历史中，将注意力引向感兴趣的一小部分信息的能力，使我们的大脑更明智地分配资源来生存、成长和社交。</p><h2 id="1-1-生物学中的注意力机制"><a href="#1-1-生物学中的注意力机制" class="headerlink" title="1.1 生物学中的注意力机制"></a>1.1 生物学中的注意力机制</h2><p>能够基于非自住性提示和自主性提示有选择地引导注意力的焦点：</p><ul><li>非自主性提示：基于环境中的物体的突出性和易见性</li><li>自主性提示：基于主观医院推动的；</li></ul><h2 id="1-2-查询、键和值"><a href="#1-2-查询、键和值" class="headerlink" title="1.2 查询、键和值"></a>1.2 查询、键和值</h2><p>通过两种注意力提示，用神经网络来设计注意力机制：</p><ul><li>在注意力机制的背景下，将自主性提示成为 <em>查询（Query）</em><ul><li>给定任何查询，注意力机制通过注意力汇聚（Attention Pooling）将选择引导至感官输入（Sensory Inputs）。</li></ul></li><li><h2 id="在注意力机制中，将感官输入称为值-Value-每个值都与一个-键-key-配对，这可以想象为感官输入的非自住提示。"><a href="#在注意力机制中，将感官输入称为值-Value-每个值都与一个-键-key-配对，这可以想象为感官输入的非自住提示。" class="headerlink" title="在注意力机制中，将感官输入称为值(Value)- 每个值都与一个 键(key) 配对，这可以想象为感官输入的非自住提示。"></a>在注意力机制中，将感官输入称为<em>值(Value)</em><br>- 每个值都与一个 <em>键(key)</em> 配对，这可以想象为感官输入的非自住提示。</h2></li></ul><p><img src="/./assets/qkv.svg" alt="qkv.svg"></p><ul><li>查询（自主提示）和键（非自主提示）之间的交互形成了注意力汇聚，注意力汇聚有选择地聚合了值（感官输入）以生成最终的输出</li></ul><h1 id="2-注意力汇聚"><a href="#2-注意力汇聚" class="headerlink" title="2 注意力汇聚"></a>2 注意力汇聚</h1><h2 id="2-1-非参数注意力汇聚：Nadaraya-Watson核回归"><a href="#2-1-非参数注意力汇聚：Nadaraya-Watson核回归" class="headerlink" title="2.1  非参数注意力汇聚：Nadaraya-Watson核回归"></a>2.1  非参数注意力汇聚：Nadaraya-Watson核回归</h2><ul><li><p>一种简单的估计器解决回归问题：基于平均汇聚来计算所有训练样本输出值的平均值：</p><ul><li><p>$$<br>f(x) &#x3D; \frac{1}{n}\sum^{n}_{i&#x3D;1}y_i<br>$$</p></li><li><p>这个估计器没有考虑输入 $x_i$</p></li></ul></li><li><p>Nadaraya 和 Waston提出了根据输入的位置对输出 $y_i$ 进行加权的方法—— <em>Nadaraya-Watson核回归</em>：</p><ul><li><p>$$<br>f(x)&#x3D;\sum^{n}<em>{i&#x3D;1}\frac{K(x-x_i)}{\sum^{n}</em>{j&#x3D;1}K(x-x_i)}<br>$$</p></li><li><p>其中 $K$ 是 <em>核(kernel)</em></p></li></ul></li><li><p>重写Nadaraya-Watson核回归形式，使其成为一个更通用的形式如下：</p><ul><li><p>$$<br>f(x)&#x3D;\sum^{n}_{x&#x3D;1}\alpha (x,x_i)y_i<br>$$</p><ul><li>其中 $x$ 是查询， $(x_i, y_i)$ 是键值对。式子2中注意力汇聚 是 $y_i$ 的加权平均。将查询 $x$ 和键 $x_i$ 之间的关系建模为 <em>注意力权重（Attention Weight</em>） ，这个权重被分配给每一个对应值 $y_i$ .</li><li>对于任何查询，模型在所有键值对注意力权重都是一个有效的概率分布： 它们是非负的，并且总和为1。</li></ul></li></ul></li><li><p>定义高斯核：</p><ul><li>$$<br>K(u)&#x3D;\frac{1}{\sqrt{2\pi}}exp(-\frac{u^2}{2})<br>$$</li></ul></li><li><p>将高斯核带入式2和式3可得：</p><ul><li><p>$$<br>\begin{split}\begin{aligned} f(x) &amp;&#x3D;\sum_{i&#x3D;1}^n \alpha(x, x_i) y_i\ &amp;&#x3D; \sum_{i&#x3D;1}^n \frac{\exp\left(-\frac{1}{2}(x - x_i)^2\right)}{\sum_{j&#x3D;1}^n \exp\left(-\frac{1}{2}(x - x_j)^2\right)} y_i \&amp;&#x3D; \sum_{i&#x3D;1}^n \mathrm{softmax}\left(-\frac{1}{2}(x - x_i)^2\right) y_i. \end{aligned}\end{split}<br>$$</p><ul><li>如果一个键 $x_i$ 越是接近给定的查询 $x$ ，那么分配给这个键对应值 $y_i$ 的注意力权重就会越大，也就获得了更多的注意力。</li></ul></li><li><p>因为 <em>Nadaraya-Watson核回归</em> 是一个非参数模型，因此式5的模型为非参数的注意力汇聚模型（nonparametric attention pooling）</p></li></ul></li></ul><h2 id="2-2-带参数注意力汇聚"><a href="#2-2-带参数注意力汇聚" class="headerlink" title="2.2 带参数注意力汇聚"></a>2.2 带参数注意力汇聚</h2><p>将可学习的参数 $w$ 集成到注意力汇聚中：<br>$$<br>\begin{split}\begin{aligned}f(x) &amp;&#x3D; \sum_{i&#x3D;1}^n \alpha(x, x_i) y_i \&amp;&#x3D; \sum_{i&#x3D;1}^n \frac{\exp\left(-\frac{1}{2}((x - x_i)w)^2\right)}{\sum_{j&#x3D;1}^n \exp\left(-\frac{1}{2}((x - x_j)w)^2\right)} y_i \&amp;&#x3D; \sum_{i&#x3D;1}^n \mathrm{softmax}\left(-\frac{1}{2}((x - x_i)w)^2\right) y_i.\end{aligned}\end{split}<br>$$</p><h1 id="3-注意力评分函数"><a href="#3-注意力评分函数" class="headerlink" title="3. 注意力评分函数"></a>3. 注意力评分函数</h1><ul><li>可以将6中的高斯核指数部分视为<em>注意力评分函数（attention scoring function）</em>，</li><li>通过式6可以得到与键对应的值的概率分布（即注意力权重）</li></ul><p><img src="/./assets/attention-output.svg" alt="../_images/attention-output.svg"></p><p>用数学语言描述，假设有一个查询 $\mathbf{q} \in \mathbb{R}^q$ 和 m个“键－值”对 $(\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}_m)$， 其中$\mathbf{k}_i \in \mathbb{R}^k$。 注意力汇聚函数f就被表示成值的加权和：</p><ul><li><p>$$<br>f(\mathbf{q}, (\mathbf{k}_1, \mathbf{v}_1), \ldots, (\mathbf{k}_m, \mathbf{v}<em>m)) &#x3D; \sum</em>{i&#x3D;1}^m \alpha(\mathbf{q}, \mathbf{k}_i) \mathbf{v}_i \in \mathbb{R}^v,<br>$$</p></li><li><p>其中查询 $q$ 和键 $k_i$ 的注意力权重（标量） 是通过注意力评分函数 $a$ 将两个向量映射成标量， 再经过softmax运算得到的：</p></li><li><p>$$<br>\alpha(\mathbf{q}, \mathbf{k}_i) &#x3D; \mathrm{softmax}(a(\mathbf{q}, \mathbf{k}_i)) &#x3D; \frac{\exp(a(\mathbf{q}, \mathbf{k}<em>i))}{\sum</em>{j&#x3D;1}^m \exp(a(\mathbf{q}, \mathbf{k}_j))} \in \mathbb{R}.<br>$$</p></li></ul><h2 id="3-1-掩蔽softmax操作"><a href="#3-1-掩蔽softmax操作" class="headerlink" title="3.1 掩蔽softmax操作"></a>3.1 掩蔽softmax操作</h2><p>softmax操作用于输出一个概率分布作为注意力权重。 在某些情况下，并非所有的值都应该被纳入到注意力汇聚中。</p><ul><li>为了仅将有意义的词元作为值来获取注意力汇聚，可以指定一个有效序列长度，以便在计算softmax时过滤超出指定范围的位置。</li><li>实现softmax操作（masked softmax operation），其中任何超出有效长度的位置都被掩蔽并置为0。</li></ul><h2 id="3-2-加性注意力"><a href="#3-2-加性注意力" class="headerlink" title="3.2 加性注意力"></a>3.2 加性注意力</h2><p>一般来说，当查询和键是不同长度的矢量时， 我们可以使用加性注意力作为评分函数。 给定查询 $\mathbf{q} \in \mathbb{R}^q$ 和 键$\mathbf{k} \in \mathbb{R}^k$， <em>加性注意力</em>（additive attention）的评分函数为:<br>$$<br>a(\mathbf q, \mathbf k) &#x3D; \mathbf w_v^\top \text{tanh}(\mathbf W_q\mathbf q + \mathbf W_k \mathbf k) \in \mathbb{R},<br>$$</p><ul><li>其中可学习的参数时 $\mathbf W_q\in\mathbb R^{h\times q}$ 、$\mathbf W_k \in \mathbb R^{h \times k}$ 和 $\mathbf w_v \in \mathbb R^h$ </li><li>将查询和键连结起来后输入到一个多层感知机（MLP）中， 感知机包含一个隐藏层，其隐藏单元数是一个超参数h。 通过使用tanh作为激活函数，并且禁用偏置项。</li></ul><h2 id="3-3-缩放点积注意力"><a href="#3-3-缩放点积注意力" class="headerlink" title="3.3 缩放点积注意力"></a>3.3 缩放点积注意力</h2><p>使用点积可以得到计算效率更高的评分函数，但是点积操作要求查询和键具有相同长度 $d$。假设查询和键的所有元素都是独立的随机变量，并满足零均值和单位方差，那么两个向量的点积的均值为0，方差为 $d$。</p><ul><li><p>为了确保无论向量长度如何，点积的方差在不考虑向量长度的情况下任然为1，将点积除以 $\sqrt{d}$ ，则得到缩放点积注意力（scaled dot-product attention）评分函数为：</p></li><li><p>$$<br>a(\mathbf q, \mathbf k) &#x3D; \mathbf{q}^\top \mathbf{k}  &#x2F;\sqrt{d}.<br>$$</p></li><li><p>在实践中通常从小批量的角度来考虑提高效率，查询和键的长度为 $d$，值的长度为 $v_0$ 。查询$\mathbf Q\in\mathbb R^{n\times d}$、 键$\mathbf K\in\mathbb R^{m\times d}$ 和 值 $\mathbf V\in\mathbb R^{m\times v}$ 的缩放点积注意力是：</p><ul><li><p>$$<br>\mathrm{softmax}\left(\frac{\mathbf Q \mathbf K^\top }{\sqrt{d}}\right) \mathbf V \in \mathbb{R}^{n\times v}.<br>$$</p></li><li></li></ul></li></ul><h1 id="4-Bahdanau注意力"><a href="#4-Bahdanau注意力" class="headerlink" title="4. Bahdanau注意力"></a>4. Bahdanau注意力</h1><p>Bahdanau等人基于Graves 的可微注意力模型，将文本字符与更长的笔迹对齐，其中对齐方式仅向一个方向移动的启发，提出了一个没有严格单项对齐限制的 <em>可微注意力模型</em>[ <a href="https://zh-v2.d2l.ai/chapter_references/zreferences.html#id6">Bahdanau et al., 2014</a>]</p><ul><li>在预测词元时，如果不是所有输入词元都相关，模型将仅对齐（或参与）输入序列中与当前预测相关的部分。<ul><li>这是通过将上下文变量视为注意力集中的输出来实现的</li></ul></li></ul><h2 id="4-1-模型"><a href="#4-1-模型" class="headerlink" title="4.1 模型"></a>4.1 模型</h2><p>这个新的基于注意力机制模型定义如下：<br>$$<br>\mathbf{c}<em>{t’} &#x3D; \sum</em>{t&#x3D;1}^T \alpha(\mathbf{s}_{t’ - 1}, \mathbf{h}_t) \mathbf{h}_t,<br>$$</p><ul><li>其中，时间步 $\mathbf t’-1$ 时的解码器器隐状态 $\mathbf s_{t’-1}$ 是查询，编码器隐状态 $\mathbf h_t$ 即是键，也是值，注意力权重 $\mathbf \alpha$ 是使用 <em>式 9</em> 所定义的加性注意力打分函数计算的</li></ul><p>其架构如下图：</p><p><img src="/./assets/seq2seq-attention-details.svg" alt="../_images/seq2seq-attention-details.svg"></p><h1 id="5-多头注意力"><a href="#5-多头注意力" class="headerlink" title="5. 多头注意力"></a>5. 多头注意力</h1><p>为了当给定相同的查询、键和值的集合时，模型可以基于相同的注意力机制学习到不同的行为，然后将不同的行为作为知识组合起来，捕获序列内各种范围的依赖（例如短距离依赖和长距离依赖关系）。因此，允许注意力机制组合起来使用查询、键和值的不同 <em>子空间表示 （ representation subspaces ）</em>时有益的。</p><ul><li>解决方法：<ul><li>可以用独立学习得到的h组不同的 <em>线性投影 ( Linear projections )</em> 来变换查询、键和值。</li><li>然后将这h组变换后的查询、键和值并行送到注意力汇聚中，以产生最终的输出。</li><li>这种方法被称为 <strong><code>多头注意力机制（multihead attention）</code></strong> <ul><li>对于h个注意力汇聚输出，每一个注意力汇聚都被称为一个 <em>头（head）</em></li></ul></li></ul></li></ul><p><img src="/./assets/multi-head-attention.svg" alt="../_images/multi-head-attention.svg"></p><h2 id="5-1-模型"><a href="#5-1-模型" class="headerlink" title="5.1 模型"></a>5.1 模型</h2><p>多头注意力机制用数学语言描述如下：</p><ul><li><p>给定查询 $\mathbf{q} \in \mathbb{R}^{d_q}$ 、键 $\mathbf{k} \in \mathbb{R}^{d_k}$ 和  值 $\mathbf{v} \in \mathbb{R}^{d_v}$，每个注意力头 $ \mathbf{h_i}\quad(i &#x3D; 1, \ldots, h)$ 的计算方式为：</p></li><li><p>$$<br>\mathbf{h}_i &#x3D; f(\mathbf W_i^{(q)}\mathbf q, \mathbf W_i^{(k)}\mathbf k,\mathbf W_i^{(v)}\mathbf v) \in \mathbb R^{p_v},<br>$$</p><ul><li><p>其中可学习的参数包括 $\mathbf W_i^{(q)}\in\mathbb R^{p_q\times d_q}$ 、$\mathbf W_i^{(k)}\in\mathbb R^{p_k\times d_k}$ 和 $\mathbf W_i^{(v)}\in\mathbb R^{p_v\times d_v}$ ，以及代表注意力汇聚的函数$f$ , $f$ 可以是3中的 加性注意力和缩放点积注意力。 多头注意力的输出需要经过另一个线性转换， 它对应着h个头连结后的结果，因此其可学习参数是 $\mathbf W_o\in\mathbb R^{p_o\times h p_v}$：</p></li><li><p>$$<br>\begin{split}\mathbf W_o \begin{bmatrix}\mathbf h_1\\vdots\\mathbf h_h\end{bmatrix} \in \mathbb{R}^{p_o}.\end{split}<br>$$</p><ul><li>基于这种设计，每个头都可能会关注输入的不同部分， 可以表示比简单加权平均值更复杂的函数。</li></ul></li></ul></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/10/20/hello-world/"/>
    <url>/2023/10/20/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
